## 排序模型

### DIN

- **DIN中的attention和Transformer中的attention有什么不同？**

答：Transformer中的attention采用缩放点积注意力，是一种固定的注意力函数形式，通过计算Q和K的点积再进行softmax的得到注意力权重
而在DIN中，注意力机制是一种可学习的非线性函数，先拼接候选物品embedding，用户行为序列中每个商品embedding以及二者元素积之后的向量，输入到一个MLP中由神经网络计算注意力权重。
